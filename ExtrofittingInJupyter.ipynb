{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement of Extrofitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn import decomposition\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "### Hyperparameter\n",
    "WordDim = 300\n",
    "ExpandDim = 1\n",
    "NormRead = True\n",
    "ExpandNorm = True\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_word_vecs(filename):\n",
    "    print(\"Vectors read from\", filename)\n",
    "    wordVectors = {}\n",
    "    fileObject = open(filename, 'r')\n",
    "    fileObject.readline() # For handling First Line\n",
    "    for line in fileObject:\n",
    "        line = line.strip().lower()\n",
    "        word = line.split()[0]\n",
    "        wordVectors[word] = np.zeros(len(line.split())-1, dtype=np.float64)\n",
    "        vector = line.split()[1:]\n",
    "        if len(vector) == WordDim:\n",
    "            for index, vecVal in enumerate(vector):\n",
    "                wordVectors[word][index] = float(vecVal)\n",
    "            if NormRead:\n",
    "                wordVectors[word] = wordVectors[word] / math.sqrt((wordVectors[word]**2).sum() + 1e-5)\n",
    "    return wordVectors\n",
    "\n",
    "isNumber = re.compile(r'\\d+')\n",
    "def norm_word(word): # Could Add Preprocessing\n",
    "    if isNumber.search(word.lower()):\n",
    "        return '---num---'\n",
    "    elif re.sub(r'\\W+', '', word) == '':\n",
    "        return '---punc---'\n",
    "    else:\n",
    "        return word.lower()\n",
    "    \n",
    "def read_lexicon(filename, wordVecs):\n",
    "    lexicon = {}\n",
    "    for line in open(filename, 'r'):\n",
    "        words = line.lower().strip().split()\n",
    "        lexicon[norm_word(words[0])] = [norm_word(word) for word in words[1:]]\n",
    "    return lexicon\n",
    "\n",
    "def wordVecsLDA(wordVecs):\n",
    "    newWordVecs = deepcopy(wordVecs)\n",
    "    wordVec_np = []\n",
    "    \n",
    "    for k in newWordVecs.keys():\n",
    "        wordVec_np.append(newWordVecs[k])\n",
    "    wordVec_np = np.array(wordVec_np)\n",
    "    \n",
    "    if ExpandNorm:\n",
    "        wordVec_np[:,-ExpandDim:-1] \\\n",
    "        = wordVec_np[:,-ExpandDim:-1] / np.sqrt(np.sum(wordVec_np[:,-ExpandDim:-1]**2, axis=0) + 1e-5)\n",
    "\n",
    "    lda = LinearDiscriminantAnalysis(n_components=WordDim)\n",
    "    wordVec_np = lda.fit_transform(wordVec_np[:,:-1], wordVec_np[:,-1])\n",
    "        \n",
    "    for i, k in enumerate(newWordVecs.keys()):\n",
    "        newWordVecs[k] = wordVec_np[i]\n",
    "    return newWordVecs\n",
    "\n",
    "def extrofit(wordVecs, lexicon, it):\n",
    "    newWordVecs = deepcopy(wordVecs)\n",
    "    wvVocab = set(newWordVecs.keys())\n",
    "    loopVocab = wvVocab.intersection(set(lexicon.keys()))\n",
    "    if it == 0: print(len(loopVocab), \"words will be extrofitted\")\n",
    "    wordidx = 0\n",
    "    for w in wvVocab:\n",
    "        for i in range(ExpandDim):\n",
    "            newWordVecs[w] = np.hstack((newWordVecs[w], np.mean(wordVecs[w])))\n",
    "        newWordVecs[w] = np.hstack((newWordVecs[w], np.zeros(1)))\n",
    "\n",
    "    for word in wvVocab:\n",
    "        wordidx = wordidx+1\n",
    "        try:\n",
    "            wordNeighbours = set(lexicon[word]).intersection(wvVocab)\n",
    "            numNeighbours = len(wordNeighbours)\n",
    "        except KeyError: numNeighbours = 0\n",
    "\n",
    "        if numNeighbours == 0:\n",
    "            newWordVecs[word][-1] = wordidx\n",
    "        else:\n",
    "            for w in wordNeighbours:\n",
    "                newWordVecs[w][-1] = wordidx\n",
    "    print(\"Dimension Reduction ... \")\n",
    "    newWordVecs = wordVecsLDA(newWordVecs)\n",
    "        \n",
    "    return newWordVecs\n",
    "\n",
    "def print_word_vecs(wordVectors, outFileName):\n",
    "    print('Writing down the vectors in', outFileName)\n",
    "    outFile = open(outFileName, 'w')\n",
    "    outFile.write(str(len(wordVectors)) + ' ' + str(WordDim) + '\\n')\n",
    "    pbar = tqdm_notebook(total = len(wordVectors), desc = 'Writing')\n",
    "    for word, values in wordVectors.iteritems():\n",
    "        pbar.update(1)\n",
    "        outFile.write(word+' ')\n",
    "        for val in wordVectors[word]:\n",
    "            outFile.write('%.5f' %(val)+' ')\n",
    "        outFile.write('\\n')\n",
    "    outFile.close()\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors read from ./word_vectors/glove.txt\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'cp949' codec can't decode byte 0xcf in position 6768: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-db672835fb3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m### Loading Pretrained Word Vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwordVecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_word_vecs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./word_vectors/glove.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m### Select Lexicon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlexicon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_lexicon\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./lexicons/ppdb-xl.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordVecs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-a17ea797535c>\u001b[0m in \u001b[0;36mread_word_vecs\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mfileObject\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mfileObject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# For handling First Line\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileObject\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'cp949' codec can't decode byte 0xcf in position 6768: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "### Loading Pretrained Word Vector\n",
    "wordVecs = read_word_vecs(\"./word_vectors/glove.txt\")\n",
    "\n",
    "### Select Lexicon\n",
    "lexicon = read_lexicon('./lexicons/ppdb-xl.txt', wordVecs)\n",
    "# lexicon = read_lexicon(\"./lexicons/wordnet-synonyms.txt\", wordVecs)\n",
    "# lexicon = read_lexicon(\"./lexicons/wordnet-synonyms+.txt\", wordVecs)\n",
    "# lexicon = read_lexicon(\"./lexicons/framenet.txt\", wordVecs)\n",
    "\n",
    "### Run\n",
    "wordVecs_extro = extrofit(wordVecs, lexicon, 0)\n",
    "print(\"Ready (>_<)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Similarity Tasks (MEN-3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import scipy.stats\n",
    "    \n",
    "\"\"\" MEN-3k \"\"\"\n",
    "fp_men = open(\"./MEN_dataset_natural_form_full.txt\", 'r')\n",
    "fp_men_ = fp_men.read().split('\\n')\n",
    "\n",
    "data_men = []\n",
    "for row in fp_men_: data_men.append(row.split(' '))\n",
    "data_men.pop() # Removing empty list\n",
    "data_men = np.array(data_men)\n",
    "fp_men.close()\n",
    "\n",
    "word_to_idx_men = {}\n",
    "\n",
    "idx = 0\n",
    "for w in data_men[:,0]:\n",
    "    try: word_to_idx_men[w]\n",
    "    except KeyError:\n",
    "        word_to_idx_men[w] = idx\n",
    "        idx = idx+1\n",
    "for w in data_men[:,1]:\n",
    "    try: word_to_idx_men[w]\n",
    "    except KeyError:\n",
    "        word_to_idx_men[w] = idx\n",
    "        idx = idx+1\n",
    "\n",
    "word_to_idx_men = sorted(word_to_idx_men.items(), key=operator.itemgetter(1))\n",
    "\n",
    "\"\"\" Calculating Similarity \"\"\"\n",
    "def LoadInputVector(wordvec, data, lookup):\n",
    "    input1, input2 = [], []\n",
    "    unk_cnt = 0\n",
    "    for i in range(len(data[:,0])):\n",
    "        try: input1.append(wordvec[data[i,0]])\n",
    "        except KeyError: input1.append(np.random.normal(0., 1., WordDim)); unk_cnt = unk_cnt+1\n",
    "        try: input2.append(wordvec[data[i,1]])\n",
    "        except KeyError: input2.append(np.random.normal(0., 1., WordDim)); unk_cnt = unk_cnt+1\n",
    "    return np.array(input1), np.array(input2)\n",
    "\n",
    "def Evaluating_MEN(wordvec, data, lookup):\n",
    "    input1, input2 = LoadInputVector(wordvec, data, lookup)\n",
    "    output = []\n",
    "    epsilon = 1e-5\n",
    "    for i in range(len(input1)):\n",
    "        output.append(np.dot(input1[i], input2[i])/(np.linalg.norm(input1[i])*np.linalg.norm(input2[i])))\n",
    "    output = (np.array(output)).reshape(-1)\n",
    "    return round(scipy.stats.spearmanr(output, np.array(data[:,2], dtype=float))[0], 4)\n",
    "\n",
    "lookup_men = dict(word_to_idx_men)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MEN-3k Dataset>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'wordVecs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d2858fcd374b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'<MEN-3k Dataset>'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Original :\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEvaluating_MEN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordVecs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_men\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlookup_men\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Extrofit :\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEvaluating_MEN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordVecs_extro\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_men\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlookup_men\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint_word_vecs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordVecs_extro\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SimpleGloveExtrofit_Dim'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mExpandDim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wordVecs' is not defined"
     ]
    }
   ],
   "source": [
    "print('<MEN-3k Dataset>')\n",
    "print(\"Original :\", Evaluating_MEN(wordVecs, data_men, lookup_men))\n",
    "print(\"Extrofit :\", Evaluating_MEN(wordVecs_extro, data_men, lookup_men))\n",
    "\n",
    "print_word_vecs(wordVecs_extro, 'SimpleGloveExtrofit_Dim' + str(ExpandDim) + '.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
